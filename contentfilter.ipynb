{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports, data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "behaviors_path = 'MINDsmall_train/behaviors.tsv'\n",
    "news_path = 'MINDsmall_train/news.tsv'\n",
    "news_data = pd.read_csv(news_path, sep='\\t', header=None, names=['ArticleID', 'Category', 'SubCategory', 'Title', 'Abstract', 'URL', 'TitleEntities', 'AbsEntities'])\n",
    "\n",
    "news_data['Title'] = news_data['Title'].fillna('')\n",
    "news_data['Abstract'] = news_data['Abstract'].fillna('')\n",
    "\n",
    "# Concatenating title and abstract for a comprehensive representation\n",
    "news_data['content'] = news_data['Title'] + \" \" + news_data['Abstract']\n",
    "\n",
    "# Vectorizing the content using TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(news_data['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading entity embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_entity_embeddings(file_path):\n",
    "    embeddings = {}\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            values = line.split()\n",
    "            entity_id = values[0]\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings[entity_id] = vector\n",
    "    return embeddings\n",
    "\n",
    "entity_embeddings_path = './MINDsmall_train/entity_embedding.vec'\n",
    "entity_embeddings = load_entity_embeddings(entity_embeddings_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def article_to_embedding(article_entities, entity_embeddings):\n",
    "    try:\n",
    "        q_entities = [entity['WikidataId'] for entity in json.loads(article_entities)]\n",
    "    except:\n",
    "        q_entities = []\n",
    "    embeddings = [entity_embeddings[entity] for entity in q_entities if entity in entity_embeddings]\n",
    "    \n",
    "    if embeddings:\n",
    "        article_embedding = np.mean(embeddings, axis=0)\n",
    "    else:\n",
    "        article_embedding = np.zeros(next(iter(entity_embeddings.values())).shape)\n",
    "    return article_embedding\n",
    "\n",
    "def embeddings_count(article_entities):\n",
    "    try:\n",
    "        return len([entity['WikidataId'] for entity in json.loads(article_entities)])\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def combine_embs_hstack(row): #Kanskje mean istedenfor hstack\n",
    "    if row[\"AbsEmbeddingsCount\"] == 0 and row[\"TitleEmbeddingsCount\"] != 0:\n",
    "        return np.hstack((row[\"TitleEmbs\"], row[\"TitleEmbs\"], row[\"TfidfEmbs\"]))\n",
    "    elif row[\"AbsEmbeddingsCount\"] != 0 and row[\"TitleEmbeddingsCount\"] == 0:\n",
    "        return np.hstack((row[\"AbsEmbs\"], row[\"AbsEmbs\"], row[\"TfidfEmbs\"]))\n",
    "    return np.hstack((row[\"TitleEmbs\"], row[\"AbsEmbs\"], row[\"TfidfEmbs\"]))\n",
    "\n",
    "\n",
    "# Compute raw embeddings\n",
    "news_data[\"TitleEmbs\"] = news_data['TitleEntities'].apply(lambda x: article_to_embedding(x, entity_embeddings))\n",
    "news_data[\"AbsEmbs\"] = news_data['AbsEntities'].apply(lambda x: article_to_embedding(x, entity_embeddings))\n",
    "\n",
    "# title_embs = np.array([article_to_embedding(entities, entity_embeddings) for entities in news_data['TitleEntities'].fillna('')])\n",
    "# abs_embs = np.array([article_to_embedding(entities, entity_embeddings) for entities in news_data['AbsEntities'].fillna('')])\n",
    "\n",
    "# Normalize embs\n",
    "normalized_title_embs = normalize(news_data[\"TitleEmbs\"].values.tolist(), axis=1)\n",
    "normalized_abs_embs = normalize(news_data[\"AbsEmbs\"].values.tolist(), axis=1)\n",
    "tfidf_normalized = normalize(tfidf_matrix.toarray(), axis=1)\n",
    "\n",
    "list_normalized_tfidf = [list(x) for x in tfidf_normalized]\n",
    "list_normalized_title_embs = [list(x) for x in normalized_title_embs]\n",
    "list_normalized_abs_embs = [list(x) for x in normalized_abs_embs]\n",
    "\n",
    "news_data[\"TitleEmbs\"] = list_normalized_title_embs\n",
    "news_data[\"AbsEmbs\"] = list_normalized_abs_embs\n",
    "news_data[\"TfidfEmbs\"] = list_normalized_tfidf\n",
    "\n",
    "# compute count\n",
    "news_data[\"AbsEmbeddingsCount\"] = news_data['AbsEntities'].apply(lambda x: embeddings_count(x))\n",
    "news_data[\"TitleEmbeddingsCount\"] = news_data['TitleEntities'].apply(lambda x: embeddings_count(x))\n",
    "\n",
    "news_data[\"CombinedEmbeddings\"] = news_data.apply(lambda x: combine_embs_hstack(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.3414373083684857,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.30330566002117565,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " ...]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_data['TfidfEmbs'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_matrix = np.stack(news_data['CombinedEmbeddings'].values)\n",
    "similarity_matrix = cosine_similarity(embeddings_matrix)\n",
    "article_ids = list(news_data['ArticleID'])\n",
    "article_index_dict = {id: index for index, id in enumerate(article_ids)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all([article_ids[id] == index for index, id in article_index_dict.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_n_closest_articles(article_id, similarity_matrix, article_ids, n=5):\n",
    "    # Get the index of the article_id in the DataFrame\n",
    "    article_idx = article_ids.index(article_id)\n",
    "    \n",
    "    # Get the similarity scores for all other articles from the similarity matrix\n",
    "    similarity_scores = similarity_matrix[article_idx]\n",
    "    \n",
    "    # Get indices of the scores sorted from highest to lowest\n",
    "    sorted_indices = np.argsort(similarity_scores)[::-1]\n",
    "    \n",
    "    # Find the top n closest article indices, ignoring the first one as it's the article itself\n",
    "    closest_indices = sorted_indices[1:n+1]\n",
    "    \n",
    "    # Retrieve the corresponding article IDs\n",
    "    closest_article_ids = [article_ids[i] for i in closest_indices]\n",
    "    \n",
    "    return closest_article_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article: Seahawks congratulate Sounders MLS Cup triumph, Category: sports, Subcategory: soccer\n",
      "Article: Fan Voices: Make them hear you, Category: sports, Subcategory: soccer_mls\n",
      "Article: Sounders stun LAFC, advance to third MLS Cup in four years, Category: sports, Subcategory: soccer\n",
      "Article: Seattle Sounders 3, Toronto FC 1: Takeaways from MLS Cup, Category: sports, Subcategory: soccer\n",
      "Article: From Hendersonville to MLS Cup: Toronto FC goalkeeper gives back to community with camp, Category: sports, Subcategory: soccer\n"
     ]
    }
   ],
   "source": [
    "target_article_id = 'N34418'\n",
    "closest_articles = find_n_closest_articles(target_article_id, similarity_matrix, article_ids, n=5)\n",
    "for article in closest_articles:\n",
    "    title = news_data[news_data['ArticleID'] == article]['Title'].values[0]\n",
    "    category = news_data[news_data['ArticleID'] == article]['Category'].values[0]\n",
    "    subcategory = news_data[news_data['ArticleID'] == article]['SubCategory'].values[0]\n",
    "    print(f\"Article: {title}, Category: {category}, Subcategory: {subcategory}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "behaviors_df = pd.read_csv(behaviors_path, sep='\\t', header=None, names=['ImpressionID', 'UserID', 'Time', 'History', 'Impressions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similarity_of_article_to_history(article_id, history_articles, similarity_matrix, article_index_dict):\n",
    "    history_indices = [article_index_dict.get(article, -1) for article in history_articles]\n",
    "    history_indices = [i for i in history_indices if i != -1]\n",
    "    if article_id in article_index_dict:\n",
    "        article_index = article_index_dict[article_id]\n",
    "        similarities = similarity_matrix[article_index, history_indices]\n",
    "        return np.mean(similarities) if len(history_indices) > 0 else 0.0\n",
    "    return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_acc(history, impressions, sim_matrix, article_index_dict):\n",
    "    impression_similarities = {}\n",
    "    for h2 in impressions:\n",
    "        sign = True if h2.split('-')[1] == '1' else False\n",
    "        id = h2.split('-')[0]\n",
    "        pred_similarity = get_similarity_of_article_to_history(id, history, sim_matrix, article_index_dict)\n",
    "        impression_similarities[id] = (pred_similarity, sign)\n",
    "    impression_similarities = sorted(impression_similarities.items(), key=lambda x: x[1][0], reverse=True)\n",
    "    return impression_similarities[0][1][1], impression_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25fd4b8c0d0f494a946cc49d2d2c905c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "res_bool = []\n",
    "res_sim = []\n",
    "\n",
    "for iter, row in tqdm(behaviors_df.iterrows()):\n",
    "    if isinstance(row['History'], float):\n",
    "        continue\n",
    "    history = row['History'].split(' ')\n",
    "    impressions = row['Impressions'].split(' ')\n",
    "    truth, simdict = check_acc(history, impressions, similarity_matrix, article_index_dict)\n",
    "    res_bool.append(truth)\n",
    "    res_sim.append(simdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top1 accuracy:  0.14588849063599757  total:  22427  out of  153727  avg imp count in top1:  17.87287644357248  random baseline is  0.055950702907680214\n",
      "Top5 accuracy:  0.46419952253019964  total:  71360  out of  153727  avg imp count in top5:  20.385089686098656  random baseline is  0.24527731184865398\n",
      "Top10 accuracy:  0.6403494506495281  total:  98439  out of  153727  avg imp count in top10:  23.812686028911305  random baseline is  0.4199442258575477\n",
      "Top15 accuracy:  0.740663643992272  total:  113860  out of  153727  avg imp count in top15:  26.165861584401895  random baseline is  0.5732660455920879\n"
     ]
    }
   ],
   "source": [
    "# how many top1 recommendations were correct\n",
    "print(\"Top1 accuracy: \", sum(res_bool) / len(res_bool), \" total: \", sum(res_bool), \" out of \", len(res_sim),\n",
    "      \" avg imp count in top1: \", np.mean([len(x) for x in res_sim if x[0][1][1]]), \n",
    "      \" random baseline is \", 1/np.mean([len(x) for x in res_sim if x[0][1][1]]))\n",
    "\n",
    "# how many top5 recommendations were correct\n",
    "top5 = 0\n",
    "top5_len = []\n",
    "for r in res_sim:\n",
    "    for i in range(min(5, len(r))):\n",
    "        if r[i][1][1]:\n",
    "            top5_len.append(len(r))\n",
    "            top5 += 1\n",
    "            break\n",
    "print(\"Top5 accuracy: \", top5 / len(res_bool), \" total: \", top5, \" out of \", len(res_sim),\n",
    "      \" avg imp count in top5: \", np.mean(top5_len),\n",
    "      \" random baseline is \", 5/np.mean(top5_len))\n",
    "\n",
    "# how many top10 recommendations were correct, careful out of bounds\n",
    "top10 = 0\n",
    "top10_len = []\n",
    "for r in res_sim:\n",
    "    for i in range(min(10, len(r))):\n",
    "        if r[i][1][1]:\n",
    "            top10_len.append(len(r))\n",
    "            top10 += 1\n",
    "            break\n",
    "print(\"Top10 accuracy: \", top10 / len(res_bool), \" total: \", top10, \" out of \", len(res_sim),\n",
    "      \" avg imp count in top10: \", np.mean(top10_len),\n",
    "      \" random baseline is \", 10/np.mean(top10_len))\n",
    "\n",
    "\n",
    "top15 = 0\n",
    "top15_len = []\n",
    "for r in res_sim:\n",
    "    for i in range(min(15, len(r))):\n",
    "        if r[i][1][1]:\n",
    "            top15_len.append(len(r))\n",
    "            top15 += 1\n",
    "            break\n",
    "print(\"Top15 accuracy: \", top15 / len(res_bool), \" total: \", top15, \" out of \", len(res_sim),\n",
    "      \" avg imp count in top15: \", np.mean(top15_len),\n",
    "      \" random baseline is \", 15/np.mean(top15_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommending articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_similar_article(history_articles, all_articles, similarity_matrix, article_index_dict, n):\n",
    "    # Filter history articles to those with valid indices in the article_index_dict\n",
    "    history_indices = [article_index_dict[article] for article in history_articles if article in article_index_dict]\n",
    "\n",
    "    if not history_indices:\n",
    "        return []\n",
    "\n",
    "    # Vectorized operation to get similarity scores for all articles against all history articles\n",
    "    similarity_scores = similarity_matrix[:, history_indices]\n",
    "    average_similarities = np.mean(similarity_scores, axis=1)\n",
    "\n",
    "    # Get valid indices and corresponding articles that are present in both all_articles and article_index_dict\n",
    "    valid_indices = [article_index_dict[article] for article in all_articles if article in article_index_dict]\n",
    "    valid_articles = [article for article in all_articles if article in article_index_dict]\n",
    "\n",
    "    # Filtered array of average similarities for valid articles\n",
    "    filtered_similarities = average_similarities[valid_indices]\n",
    "\n",
    "    # Sort valid articles based on filtered average similarity in descending order\n",
    "    sorted_indices = np.argsort(-filtered_similarities)\n",
    "    top_indices = sorted_indices[:n]\n",
    "    top_articles = [(valid_articles[i], filtered_similarities[top_indices[i]]) for i in range(len(top_indices))]\n",
    "\n",
    "    return top_articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kanskje visualisere likhet mellom alle i historien"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "382a2831469a4b6bb50aba1cd0c07fb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "interactions_preds_dict = {}\n",
    "all_articles = set(news_data['ArticleID'].unique())\n",
    "\n",
    "#randomly sample 10000 interactions\n",
    "\n",
    "for iter, row in tqdm(behaviors_df.sample(10000, random_state=42).iterrows()):\n",
    "    if isinstance(row['History'], float):\n",
    "        continue\n",
    "    interactionid = row['ImpressionID']\n",
    "    history = row['History'].split(' ')\n",
    "    impressions = row['Impressions'].split(' ')\n",
    "    sorted_articles = recommend_similar_article(history, all_articles, similarity_matrix, article_index_dict, 40)\n",
    "    interactions_preds_dict[interactionid] = (sorted_articles, impressions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "recs = []\n",
    "for interactionid, (preds, impressions) in interactions_preds_dict.items():\n",
    "    for article, predicted_sim in preds:\n",
    "        if f'{article}-1' in impressions or f'{article}-0' in impressions:\n",
    "            recs.append((interactionid, article, predicted_sim, 1 if f'{article}-1' in impressions else 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "539\n",
      "0.059369202226345084\n"
     ]
    }
   ],
   "source": [
    "print(sum([x[3] for x in recs]))\n",
    "print(len(recs))\n",
    "print(sum([x[3] for x in recs]) / len(recs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n",
      "196\n",
      "0.09693877551020408\n"
     ]
    }
   ],
   "source": [
    "recs_10 = []\n",
    "for interactionid, (preds, impressions) in interactions_preds_dict.items():\n",
    "    for article, predicted_sim in preds[:10]:\n",
    "        if f'{article}-1' in impressions or f'{article}-0' in impressions:\n",
    "            recs_10.append((interactionid, article, predicted_sim, 1 if f'{article}-1' in impressions else 0))\n",
    "print(sum([x[3] for x in recs_10]))\n",
    "print(len(recs_10))\n",
    "print(sum([x[3] for x in recs_10]) / len(recs_10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking predicted similarities vs all impressions and history, individually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80193c9eb7424a218f76bf0d0a746391",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "similarity_hist_imp = {}\n",
    "\n",
    "for iter, row in tqdm(behaviors_df.iterrows()):\n",
    "    if isinstance(row['History'], float):\n",
    "        continue\n",
    "    interactionid = row['ImpressionID']\n",
    "    history = row['History'].split(' ')\n",
    "    impressions = row['Impressions'].split(' ')\n",
    "    # get each article similarity to history\n",
    "    hist_idx = [article_index_dict.get(article) for article in history if article in article_index_dict]\n",
    "    imp_idx = [article_index_dict.get(article.split('-')[0]) for article in impressions if article.split('-')[0] in article_index_dict]\n",
    "\n",
    "    hist_imp_sim = similarity_matrix[hist_idx, :][:, imp_idx]\n",
    "    \n",
    "    similarity_hist_imp[interactionid] = hist_imp_sim\n",
    "    if iter == 10000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "checking_sim = []\n",
    "for i, row in behaviors_df.iterrows():\n",
    "    iids = []\n",
    "    impression_id = row['ImpressionID']\n",
    "    history = row['History']\n",
    "    if isinstance(history, float):\n",
    "        continue\n",
    "    for impression in row['Impressions'].split(' '):\n",
    "        # if impression.split('-')[1] == '1':\n",
    "        iids.append(impression)\n",
    "    \n",
    "    for iid in iids:\n",
    "        iid_idx = article_index_dict.get(iid.split('-')[0])\n",
    "        chosen = int(iid.split('-')[1])\n",
    "        sims = []\n",
    "        for h in history.split(' '):\n",
    "            hist_idx = article_index_dict.get(h)\n",
    "            # print(f\"Now checking: {iid} and {h}, similarity: {similarity_matrix[iid_idx, hist_idx]}, iid_idx: {iid_idx}, hist_idx: {hist_idx}\")\n",
    "            # break\n",
    "            sims.append(similarity_matrix[iid_idx, hist_idx])\n",
    "        checking_sim.append((impression_id, sims, chosen, np.mean(sims), np.median(sims), np.max(sims), np.min(sims)))\n",
    "    if i == 2000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "checking_sim_chosen = [x for x in checking_sim if x[2] == 1]\n",
    "# plot the similarity-progression in the history for the chosen articles, checking_sim_chosen[n][1] is the list of similarities\n",
    "chosen_sims = {} \n",
    "for chosen_sim in checking_sim_chosen:\n",
    "    for i, sim in enumerate(chosen_sim[1]):\n",
    "        if i not in chosen_sims:\n",
    "            chosen_sims[i] = [sim]\n",
    "        chosen_sims[i].append(sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen average mean:  0.12220226161141695\n",
      "Chosen average median:  0.1097030126364747\n",
      "Chosen average max:  0.3598597403552351\n",
      "Chosen average min:  -0.052103100464929795\n",
      "Not chosen average mean:  0.09639943552229116\n",
      "Not chosen average median:  0.08533578509223544\n",
      "Not chosen average max:  0.29917559162290674\n",
      "Not chosen average min:  -0.05192935092406395\n",
      "\n",
      "Chosen first history mean:  0.11353611625340301\n",
      "Chosen first history median:  0.06319438214486366\n",
      "Chosen first history max:  1.0\n",
      "Chosen first history min:  -0.2224091287965253\n",
      "\n",
      "Chosen last history mean:  0.12263551495478624\n",
      "Chosen last history median:  0.07441929792132591\n",
      "Chosen last history max:  1.0\n",
      "Chosen last history min:  -0.2853365833829339\n",
      "\n",
      "Chosen first half history mean:  0.12114822759450043\n",
      "Chosen first half history median:  0.0764871071490434\n",
      "Chosen first half history max:  1.0000000000000002\n",
      "Chosen first half history min:  -0.4101518084216964\n",
      "\n",
      "Chosen second half history mean:  0.12264855649817724\n",
      "Chosen second half history median:  0.07743845830638024\n",
      "Chosen second half history max:  1.0000000000000004\n",
      "Chosen second half history min:  -0.4264989658014694\n"
     ]
    }
   ],
   "source": [
    "chosen_avg_mean = []\n",
    "chosen_avg_median = []\n",
    "chosen_avg_max = []\n",
    "chosen_avg_min = []\n",
    "\n",
    "not_chosen_avg_mean = []\n",
    "not_chosen_avg_median = []\n",
    "not_chosen_avg_max = []\n",
    "not_chosen_avg_min = []\n",
    "\n",
    "\n",
    "chosen_first_history_vals = []\n",
    "chosen_last_history_vals = []\n",
    "\n",
    "chosen_first_half_hist_vals = []\n",
    "chosen_second_half_hist_vals = []\n",
    "\n",
    "for sim in checking_sim:\n",
    "    if sim[2] == 1:\n",
    "        chosen_avg_mean.append(sim[3])\n",
    "        chosen_avg_median.append(sim[4])\n",
    "        chosen_avg_max.append(sim[5])\n",
    "        chosen_avg_min.append(sim[6])\n",
    "        chosen_first_history_vals.append(sim[1][0])\n",
    "        chosen_last_history_vals.append(sim[1][-1])\n",
    "        chosen_first_half_hist_vals.extend(sim[1][:len(sim[1])//2])\n",
    "        chosen_second_half_hist_vals.extend(sim[1][len(sim[1])//2:])\n",
    "    else:\n",
    "        not_chosen_avg_mean.append(sim[3])\n",
    "        not_chosen_avg_median.append(sim[4])\n",
    "        not_chosen_avg_max.append(sim[5])\n",
    "        not_chosen_avg_min.append(sim[6])\n",
    "\n",
    "print(\"Chosen average mean: \", np.mean(chosen_avg_mean))\n",
    "print(\"Chosen average median: \", np.mean(chosen_avg_median))\n",
    "print(\"Chosen average max: \", np.mean(chosen_avg_max))\n",
    "print(\"Chosen average min: \", np.mean(chosen_avg_min))\n",
    "\n",
    "print(\"Not chosen average mean: \", np.mean(not_chosen_avg_mean))\n",
    "print(\"Not chosen average median: \", np.mean(not_chosen_avg_median))\n",
    "print(\"Not chosen average max: \", np.mean(not_chosen_avg_max))\n",
    "print(\"Not chosen average min: \", np.mean(not_chosen_avg_min))\n",
    "\n",
    "print()\n",
    "print(\"Chosen first history mean: \", np.mean(chosen_first_history_vals))\n",
    "print(\"Chosen first history median: \", np.median(chosen_first_history_vals))\n",
    "print(\"Chosen first history max: \", np.max(chosen_first_history_vals))\n",
    "print(\"Chosen first history min: \", np.min(chosen_first_history_vals))\n",
    "\n",
    "print()\n",
    "print(\"Chosen last history mean: \", np.mean(chosen_last_history_vals))\n",
    "print(\"Chosen last history median: \", np.median(chosen_last_history_vals))\n",
    "print(\"Chosen last history max: \", np.max(chosen_last_history_vals))\n",
    "print(\"Chosen last history min: \", np.min(chosen_last_history_vals))\n",
    "\n",
    "print()\n",
    "print(\"Chosen first half history mean: \", np.mean(chosen_first_half_hist_vals))\n",
    "print(\"Chosen first half history median: \", np.median(chosen_first_half_hist_vals))\n",
    "print(\"Chosen first half history max: \", np.max(chosen_first_half_hist_vals))\n",
    "print(\"Chosen first half history min: \", np.min(chosen_first_half_hist_vals))\n",
    "\n",
    "print()\n",
    "print(\"Chosen second half history mean: \", np.mean(chosen_second_half_hist_vals))\n",
    "print(\"Chosen second half history median: \", np.median(chosen_second_half_hist_vals))\n",
    "print(\"Chosen second half history max: \", np.max(chosen_second_half_hist_vals))\n",
    "print(\"Chosen second half history min: \", np.min(chosen_second_half_hist_vals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bidx_hist = article_index_dict.get('N55189')\n",
    "idx_iid = article_index_dict.get('N55689')\n",
    "similarity_matrix[idx_iid, bidx_hist] == similarity_matrix[bidx_hist, idx_iid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "[[ 0.20304136  0.34363462]\n",
      " [ 0.28050044  0.24382612]\n",
      " [ 0.12461182 -0.02835792]\n",
      " [ 0.18872453  0.36146383]\n",
      " [ 0.04357624  0.        ]\n",
      " [ 0.          0.        ]\n",
      " [ 0.18503037  0.14891398]\n",
      " [ 0.29415855  0.19921355]\n",
      " [ 0.14528373  0.28617642]]\n"
     ]
    }
   ],
   "source": [
    "print(len(similarity_hist_imp[1]))\n",
    "print(similarity_hist_imp[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = behaviors_df['UserID'].unique()\n",
    "user_histories = {}\n",
    "for user in users:\n",
    "    histories = behaviors_df[behaviors_df['UserID'] == user].History.values\n",
    "    user_histories[user] = histories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "equal = 0\n",
    "not_eq = 0\n",
    "not_eqs = []\n",
    "for u_hist in user_histories.values():\n",
    "    if not all(x == u_hist[0] for x in u_hist):\n",
    "        not_eq += 1\n",
    "        not_eqs.append(u_hist)\n",
    "    else:\n",
    "        equal += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = [x for x in not_eqs if not all(y for y in x if not isinstance(y, float))]\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evals\n",
    "#### Making testset from the last day of data (typically how data is gathered in industry), and adding prev impressions to history for each user "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30270 126695\n"
     ]
    }
   ],
   "source": [
    "behaviors_df[\"Time\"] = pd.to_datetime(behaviors_df[\"Time\"])\n",
    "last_day = behaviors_df[\"Time\"].max().date()\n",
    "\n",
    "test_df = behaviors_df[behaviors_df[\"Time\"].dt.date == last_day]\n",
    "train_df = behaviors_df[behaviors_df[\"Time\"].dt.date != last_day]\n",
    "print(len(test_df), len(train_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_user_history_sets(df):\n",
    "    user_histories = {}\n",
    "    for user in df['UserID'].unique():\n",
    "        hist_set = set()\n",
    "        histories = df[df['UserID'] == user].History.values\n",
    "        if isinstance(histories[0], float):\n",
    "            continue\n",
    "        histories = [history.split(' ') for history in histories]\n",
    "        hist_list = [hist for history in histories for hist in history]\n",
    "        impressions = df[df['UserID'] == user].Impressions.values\n",
    "        impressions = [impression.split(' ') for impression in impressions]\n",
    "        impressions = [imp for impression in impressions for imp in impression]\n",
    "        imps = [impression.split('-')[0] if impression.split('-')[1] == '1' else None for impression in impressions]\n",
    "        imps = [imp for imp in imps if imp is not None]\n",
    "        hist_set.update(imps)\n",
    "        hist_set.update(hist_list)\n",
    "        if user in user_histories:\n",
    "            user_histories[user].update(hist_set)\n",
    "        else:\n",
    "            user_histories[user] = hist_set\n",
    "    return user_histories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/g1/sz_6p5c50mb_11wqn0n0dyzr0000gn/T/ipykernel_73724/943936606.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df['UserHistory'] = test_df['UserID'].apply(lambda x: ' '.join(list(user_histories[x])) if x in user_histories else '')\n"
     ]
    }
   ],
   "source": [
    "user_histories = get_all_user_history_sets(train_df)\n",
    "test_df['UserHistory'] = test_df['UserID'].apply(lambda x: ' '.join(list(user_histories[x])) if x in user_histories else '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImpressionID</th>\n",
       "      <th>UserID</th>\n",
       "      <th>Time</th>\n",
       "      <th>History</th>\n",
       "      <th>Impressions</th>\n",
       "      <th>UserHistory</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>U73700</td>\n",
       "      <td>2019-11-14 07:01:48</td>\n",
       "      <td>N10732 N25792 N7563 N21087 N41087 N5445 N60384...</td>\n",
       "      <td>N50014-0 N23877-0 N35389-0 N49712-0 N16844-0 N...</td>\n",
       "      <td>N62058 N24233 N25792 N47817 N33164 N60384 N188...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>U89744</td>\n",
       "      <td>2019-11-14 08:38:04</td>\n",
       "      <td>N24422 N25287 N39121 N41777 N58226 N119 N29197...</td>\n",
       "      <td>N47572-0 N45523-0 N64560-0 N53245-0 N8509-0 N5...</td>\n",
       "      <td>N62058 N29197 N28088 N45794 N52631 N13057 N260...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>U29155</td>\n",
       "      <td>2019-11-14 12:26:47</td>\n",
       "      <td>N60785 N11885 N38939 N25114 N44984 N4830 N2068...</td>\n",
       "      <td>N44698-0 N37204-0 N36612-0 N64174-0 N29212-0 N...</td>\n",
       "      <td>N18708 N60785 N25114 N6890 N59139 N12576 N2386...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>U70879</td>\n",
       "      <td>2019-11-14 10:45:51</td>\n",
       "      <td>N47823 N44013 N17354 N26531 N22570 N16215 N298...</td>\n",
       "      <td>N38442-0 N50601-0 N36016-0 N42457-0 N23446-0 N...</td>\n",
       "      <td>N62058 N29197 N3388 N50890 N3086 N14029 N15788...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>40</td>\n",
       "      <td>U27024</td>\n",
       "      <td>2019-11-14 14:24:04</td>\n",
       "      <td>N38629 N50155 N29177 N56426 N63842 N36565 N307...</td>\n",
       "      <td>N20394-0 N28072-0 N29212-0 N47572-0 N54321-0 N...</td>\n",
       "      <td>N7242 N4607 N42801 N50155 N39634 N45794 N32098...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156947</th>\n",
       "      <td>156948</td>\n",
       "      <td>U30039</td>\n",
       "      <td>2019-11-14 11:44:21</td>\n",
       "      <td>N39556 N932 N13079 N26729 N14454 N60615 N24002...</td>\n",
       "      <td>N10960-0 N29369-0 N53515-0 N20676-0 N23814-0 N...</td>\n",
       "      <td>N11005 N34004 N40716 N41375 N28088 N38179 N476...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156959</th>\n",
       "      <td>156960</td>\n",
       "      <td>U72015</td>\n",
       "      <td>2019-11-14 16:20:44</td>\n",
       "      <td>N53895 N48715 N5469</td>\n",
       "      <td>N14478-0 N9621-0 N22257-0 N23391-0 N61595-0 N2...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156960</th>\n",
       "      <td>156961</td>\n",
       "      <td>U21593</td>\n",
       "      <td>2019-11-14 22:24:05</td>\n",
       "      <td>N7432 N58559 N1954 N43353 N14343 N13008 N28833...</td>\n",
       "      <td>N2235-0 N22975-0 N64037-0 N47652-0 N11378-0 N4...</td>\n",
       "      <td>N5831 N25114 N4607 N42470 N63003 N19347 N61773...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156962</th>\n",
       "      <td>156963</td>\n",
       "      <td>U75630</td>\n",
       "      <td>2019-11-14 10:58:13</td>\n",
       "      <td>N29898 N59704 N4408 N9803 N53644 N26103 N812 N...</td>\n",
       "      <td>N55913-0 N62318-0 N53515-0 N10960-0 N9135-0 N5...</td>\n",
       "      <td>N62058 N18708 N5283 N14029 N41375 N40207 N2610...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156964</th>\n",
       "      <td>156965</td>\n",
       "      <td>U64800</td>\n",
       "      <td>2019-11-14 15:25:49</td>\n",
       "      <td>N22997 N48742</td>\n",
       "      <td>N61233-0 N33828-1 N19661-0 N41934-0</td>\n",
       "      <td>N22997 N48742 N33619</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30270 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ImpressionID  UserID                Time  \\\n",
       "2                  3  U73700 2019-11-14 07:01:48   \n",
       "10                11  U89744 2019-11-14 08:38:04   \n",
       "13                14  U29155 2019-11-14 12:26:47   \n",
       "20                21  U70879 2019-11-14 10:45:51   \n",
       "39                40  U27024 2019-11-14 14:24:04   \n",
       "...              ...     ...                 ...   \n",
       "156947        156948  U30039 2019-11-14 11:44:21   \n",
       "156959        156960  U72015 2019-11-14 16:20:44   \n",
       "156960        156961  U21593 2019-11-14 22:24:05   \n",
       "156962        156963  U75630 2019-11-14 10:58:13   \n",
       "156964        156965  U64800 2019-11-14 15:25:49   \n",
       "\n",
       "                                                  History  \\\n",
       "2       N10732 N25792 N7563 N21087 N41087 N5445 N60384...   \n",
       "10      N24422 N25287 N39121 N41777 N58226 N119 N29197...   \n",
       "13      N60785 N11885 N38939 N25114 N44984 N4830 N2068...   \n",
       "20      N47823 N44013 N17354 N26531 N22570 N16215 N298...   \n",
       "39      N38629 N50155 N29177 N56426 N63842 N36565 N307...   \n",
       "...                                                   ...   \n",
       "156947  N39556 N932 N13079 N26729 N14454 N60615 N24002...   \n",
       "156959                                N53895 N48715 N5469   \n",
       "156960  N7432 N58559 N1954 N43353 N14343 N13008 N28833...   \n",
       "156962  N29898 N59704 N4408 N9803 N53644 N26103 N812 N...   \n",
       "156964                                      N22997 N48742   \n",
       "\n",
       "                                              Impressions  \\\n",
       "2       N50014-0 N23877-0 N35389-0 N49712-0 N16844-0 N...   \n",
       "10      N47572-0 N45523-0 N64560-0 N53245-0 N8509-0 N5...   \n",
       "13      N44698-0 N37204-0 N36612-0 N64174-0 N29212-0 N...   \n",
       "20      N38442-0 N50601-0 N36016-0 N42457-0 N23446-0 N...   \n",
       "39      N20394-0 N28072-0 N29212-0 N47572-0 N54321-0 N...   \n",
       "...                                                   ...   \n",
       "156947  N10960-0 N29369-0 N53515-0 N20676-0 N23814-0 N...   \n",
       "156959  N14478-0 N9621-0 N22257-0 N23391-0 N61595-0 N2...   \n",
       "156960  N2235-0 N22975-0 N64037-0 N47652-0 N11378-0 N4...   \n",
       "156962  N55913-0 N62318-0 N53515-0 N10960-0 N9135-0 N5...   \n",
       "156964                N61233-0 N33828-1 N19661-0 N41934-0   \n",
       "\n",
       "                                              UserHistory  \n",
       "2       N62058 N24233 N25792 N47817 N33164 N60384 N188...  \n",
       "10      N62058 N29197 N28088 N45794 N52631 N13057 N260...  \n",
       "13      N18708 N60785 N25114 N6890 N59139 N12576 N2386...  \n",
       "20      N62058 N29197 N3388 N50890 N3086 N14029 N15788...  \n",
       "39      N7242 N4607 N42801 N50155 N39634 N45794 N32098...  \n",
       "...                                                   ...  \n",
       "156947  N11005 N34004 N40716 N41375 N28088 N38179 N476...  \n",
       "156959                                                     \n",
       "156960  N5831 N25114 N4607 N42470 N63003 N19347 N61773...  \n",
       "156962  N62058 N18708 N5283 N14029 N41375 N40207 N2610...  \n",
       "156964                               N22997 N48742 N33619  \n",
       "\n",
       "[30270 rows x 6 columns]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making predictions on each impression in test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e0fd366735243bcb9c0efbfda41ae53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[110], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m history \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUserHistory\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     11\u001b[0m impressions \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mImpressions\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m sorted_articles \u001b[38;5;241m=\u001b[39m \u001b[43mrecommend_similar_article\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_articles\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msimilarity_matrix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marticle_index_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m40\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m interactions_preds_dict[interactionid] \u001b[38;5;241m=\u001b[39m (sorted_articles, impressions)\n",
      "Cell \u001b[0;32mIn[15], line 10\u001b[0m, in \u001b[0;36mrecommend_similar_article\u001b[0;34m(history_articles, all_articles, similarity_matrix, article_index_dict, n)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Vectorized operation to get similarity scores for all articles against all history articles\u001b[39;00m\n\u001b[1;32m      9\u001b[0m similarity_scores \u001b[38;5;241m=\u001b[39m similarity_matrix[:, history_indices]\n\u001b[0;32m---> 10\u001b[0m average_similarities \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43msimilarity_scores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Get valid indices and corresponding articles that are present in both all_articles and article_index_dict\u001b[39;00m\n\u001b[1;32m     13\u001b[0m valid_indices \u001b[38;5;241m=\u001b[39m [article_index_dict[article] \u001b[38;5;28;01mfor\u001b[39;00m article \u001b[38;5;129;01min\u001b[39;00m all_articles \u001b[38;5;28;01mif\u001b[39;00m article \u001b[38;5;129;01min\u001b[39;00m article_index_dict]\n",
      "File \u001b[0;32m/opt/miniconda3/envs/recsys/lib/python3.11/site-packages/numpy/core/fromnumeric.py:3380\u001b[0m, in \u001b[0;36m_mean_dispatcher\u001b[0;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[1;32m   3365\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3366\u001b[0m \u001b[38;5;124;03m    Round an array to the given number of decimals.\u001b[39;00m\n\u001b[1;32m   3367\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3375\u001b[0m \n\u001b[1;32m   3376\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   3377\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapfunc(a, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mround\u001b[39m\u001b[38;5;124m'\u001b[39m, decimals\u001b[38;5;241m=\u001b[39mdecimals, out\u001b[38;5;241m=\u001b[39mout)\n\u001b[0;32m-> 3380\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_mean_dispatcher\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   3381\u001b[0m                      where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   3382\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (a, where, out)\n\u001b[1;32m   3385\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_mean_dispatcher)\n\u001b[1;32m   3386\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmean\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue, \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   3387\u001b[0m          where\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "interactions_preds_dict = {}\n",
    "all_articles = set(news_data['ArticleID'].unique())\n",
    "\n",
    "#randomly sample 10000 interactions\n",
    "\n",
    "for iter, row in tqdm(test_df.sample(200, random_state=42).iterrows()):\n",
    "    if row['UserHistory'] == '':\n",
    "        continue\n",
    "    interactionid = row['ImpressionID']\n",
    "    history = row['UserHistory'].split(' ')\n",
    "    impressions = row['Impressions'].split(' ')\n",
    "    sorted_articles = recommend_similar_article(history, all_articles, similarity_matrix, article_index_dict, 40)\n",
    "    interactions_preds_dict[interactionid] = (sorted_articles, impressions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recs = []\n",
    "for interactionid, (preds, impressions) in interactions_preds_dict.items():\n",
    "    for article, predicted_sim in preds:\n",
    "        if f'{article}-1' in impressions or f'{article}-0' in impressions:\n",
    "            recs.append((interactionid, article, predicted_sim, 1 if f'{article}-1' in impressions else 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum([x[3] for x in recs]))\n",
    "print(len(recs))\n",
    "print(sum([x[3] for x in recs]) / len(recs))\n",
    "recs_10 = []\n",
    "for interactionid, (preds, impressions) in interactions_preds_dict.items():\n",
    "    for article, predicted_sim in preds[:10]:\n",
    "        if f'{article}-1' in impressions or f'{article}-0' in impressions:\n",
    "            recs_10.append((interactionid, article, predicted_sim, 1 if f'{article}-1' in impressions else 0))\n",
    "print(sum([x[3] for x in recs_10]))\n",
    "print(len(recs_10))\n",
    "print(sum([x[3] for x in recs_10]) / len(recs_10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recsys",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
